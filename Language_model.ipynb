{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e837ce8",
   "metadata": {},
   "source": [
    "# Language Models Using N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27e51c",
   "metadata": {},
   "source": [
    "As with all statistical models, the true data generating process is unknown to us, so all we can do is **estimate** the probabilities of sentences. For example, one might estimate the probability of a sentence as simply the product of the empirical probabilities (i.e., the number of times a word is observed in a dataset divided by the number of words in that dataset). In the above example, we may have:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile})$$\n",
    "\n",
    "Using this simple statistic equation, I will create a model that generates human-understandable sentence, N-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a48d5",
   "metadata": {},
   "source": [
    "## Definition of N-gram\n",
    "N-gram is a sequence of the N-words. a 2-gram (bigram) is a two word sequence of words like \"give me\" or \"broken vessels\" and a 3-gram (trigram) is a three word-sequence of words such as \"give me money\" or \"need broken vessels\". <br/>\n",
    "\n",
    "With the equation given above, I will estimate the probability of the last word of an n-gram given the previous words and use it to generate sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff46dc",
   "metadata": {},
   "source": [
    "## Project Catalog\n",
    "\n",
    "- [Part 1: Preparing the Corpus](#part1)\n",
    "- [Part 2: Tokenizing the Corpus](#part2)\n",
    "- [Part 3: Creating N-gram Model](#part3)\n",
    "- [Part 4: Testing N-gram Model](#part4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dec6dc",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3392955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1b9b6",
   "metadata": {},
   "source": [
    "## Part 1: Preparing the Corpus \n",
    "<a name='part1'></a>\n",
    "\n",
    "I'll use the `requests` module to download the \"Plain Text UTF-8\" text of a public domain book from [Project Gutenberg](https://www.gutenberg.org/) and prepare it for analysis in later questions. For instance, the book Beowulf's \"Plain Text UTF-8\" URL is [here](https://www.gutenberg.org/ebooks/16328.txt.utf-8), which can be accessed by clicking the \"Plain Text UTF-8\" link [here](https://www.gutenberg.org/ebooks/16328). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1cc5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the content of a book from url through HTTP request\n",
    "def get_book(url):\n",
    "    text = requests.get(url).text\n",
    "    title = re.findall(r'Title: ([A-Za-z ]+)', text)[0].upper()\n",
    "    pattern = r'\\*{3} START OF (?:THE|THIS) PROJECT GUTENBERG EBOOK [\\r\\n \\w]+ \\*{3}((?s).*)\\*{3} END OF (?:THE|THIS) PROJECT GUTENBERG EBOOK [\\r\\n \\w]+ \\*{3}'\n",
    "    content = re.findall(pattern, text)[0]\n",
    "    return re.sub(r'\\r\\n', '\\n', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2038e4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\t\\t\\t   The Great Gatsby\\n\\t\\t\\t\\t  by\\n\\t\\t\\t F. Scott Fitzgerald\\n\\n\\n                           Table of Contents\\n\\nI\\nII\\nIII\\nIV\\nV\\nVI\\nVII\\nVIII\\nIX\\n\\n\\n                              Once again\\n                                  to\\n                                 Zelda\\n\\n  Then wear the gold hat, if that will move her;\\n  If you can bounce high, bounce for her too,\\n  Till she cry “Lover, gold-hatted, high-bouncing lover,\\n  I must have you!”\\n\\n  Thomas Parke d’Invilliers\\n\\n\\n                                  I\\n\\nIn my younger and more vulnerable years my father gave me some advice\\nthat I’ve been turning over in my mind ever since.\\n\\n“Whenever you feel like criticizing anyone,” he told me, “just\\nremember that all the people in this world haven’t had the advantages\\nthat you’ve had.”\\n\\nHe didn’t say any more, but we’ve always been unusually communicative\\nin a reserved way, and I understood that he meant a great deal more\\nthan that. In consequence, I’m inclined to reserve all judgements, a\\nhabit that has opened up'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function with a book called The Great Gatsby\n",
    "great_gatsby = get_book('https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
    "\n",
    "great_gatsby[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec9be1",
   "metadata": {},
   "source": [
    "## Part 2: Tokenizing the Corpus\n",
    "<a name='part2'></a>\n",
    "\n",
    "Now, **tokenize** the text by implementing the function `tokenize`, which takes in a string, `book_string`, and returns a **list of the tokens** (words, numbers, and all punctuation) in the book such that:\n",
    "\n",
    "* The start of every paragraph is represented in the list with the single character `'\\x02'` (standing for START).\n",
    "* The end of every paragraph is represented in the list with the single character `'\\x03'` (standing for STOP).\n",
    "* Tokens include *no* whitespace.\n",
    "* Two or more newlines count as a paragraph break, and whitespace (e.g. multiple newlines) between two paragraphs of text do not appear as tokens.\n",
    "* All punctuation marks count as tokens, even if they are uncommon (e.g. `'@'`, `'+'`, and `'%'` are all valid tokens).\n",
    "\n",
    "For example, consider the following excerpt. (The first sentence is at the end of a larger paragraph, and the second sentence is at the start of a longer paragraph.)\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call!!\n",
    "...\n",
    "```\n",
    "Tokenizes to:\n",
    "```py\n",
    "[...\n",
    "'My', 'phone', \"'\", 's', 'dead', '.', '\\x03', '\\x02', 'I', 'didn', \"'\", 't', 'get', 'your', 'call', '!', '!'\n",
    "...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a171b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the given book text\n",
    "def tokenize(book_string):\n",
    "    book_string = '\\x02'+book_string.strip()+'\\x03'\n",
    "    book_string = re.sub('^\\n{2,}', '\\x02', book_string)\n",
    "    book_string = re.sub('\\n{2,}$', '\\x03', book_string)\n",
    "    book_string = re.sub('\\n{2,}', '\\x03\\x02', book_string)\n",
    "    pattern = r'[A-Za-z]+|[^\\s\\d\\w]|\\x03|\\x02'\n",
    "    return re.findall(pattern, book_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f9c16da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\x02', 'The', 'Great', 'Gatsby', 'by', 'F', '.', 'Scott',\n",
       "       'Fitzgerald', '\\x03', '\\x02', 'Table', 'of', 'Contents', '\\x03',\n",
       "       '\\x02', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX',\n",
       "       '\\x03', '\\x02', 'Once', 'again', 'to', 'Zelda', '\\x03', '\\x02',\n",
       "       'Then', 'wear', 'the', 'gold', 'hat', ',', 'if', 'that', 'will',\n",
       "       'move', 'her', ';', 'If', 'you', 'can', 'bounce', 'high', ',',\n",
       "       'bounce', 'for', 'her', 'too', ',', 'Till', 'she', 'cry', '“',\n",
       "       'Lover', ',', 'gold', '-', 'hatted', ',', 'high', '-', 'bouncing',\n",
       "       'lover', ',', 'I', 'must', 'have', 'you', '!', '”', '\\x03', '\\x02',\n",
       "       'Thomas', 'Parke', 'd', '’', 'Invilliers', '\\x03', '\\x02', 'I',\n",
       "       '\\x03', '\\x02', 'In', 'my', 'younger', 'and', 'more', 'vulnerable',\n",
       "       'years', 'my', 'father', 'gave', 'me'], dtype='<U17')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on The Great Gatsby\n",
    "tokenized = tokenize(great_gatsby)\n",
    "np.array(tokenized)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438b9eb",
   "metadata": {},
   "source": [
    "## Part 3: Creating N-Gram Model\n",
    "<a name='part3'></a>\n",
    "Sentences are built from tokens, and the likelihood that a token occurs where it does depends on the tokens before it. This points to using **conditional probability** to compute $P(w)$. That is, we can write:\n",
    "\n",
    "$$\n",
    "P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})\n",
    "$$  \n",
    "Using **chain rule** for probabilities.\n",
    "\n",
    "**Example:** \n",
    "\n",
    "<center><code>'when I drink Coke I smile'</code></center>\n",
    "    \n",
    "The probability that it occurs, according the the chain rule, is\n",
    "\n",
    "$$\n",
    "P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I})\\cdot P(\\text{Coke | when I drink}) \\cdot P(\\text{I | when I drink Coke}) \\cdot P(\\text{smile | when I drink Coke I})\n",
    "$$\n",
    "\n",
    "That is, the probability that the sentence occurs is the product of the probability that each subsequent token follows the tokens that came before. For example, the probability $P(\\text{Coke | when I drink})$ is likely pretty high, as Coke is something that you drink. The probability $P(\\text{pizza | when I drink})$ is likely low, because pizza is not something that you drink.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620749f",
   "metadata": {},
   "source": [
    "### Side Note 1: Uniform Language Models\n",
    "\n",
    "A uniform language model is one in which each **unique** token is equally likely to appear in any position, unconditional of any other information. In other words, in a uniform language model, the probability assigned to each token is **1 over the total number of unique tokens in the corpus**.\n",
    "\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "The example corpus above has 14 **unique** tokens. This means that I'd have $P(\\text{\\x02}) = \\frac{1}{14}$, $P(\\text{when}) = \\frac{1}{14}$, and so on. Specifically, in this example, **the Series that `train` returns should contain the following values**:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{14}$ |\n",
    "| `'when'` | $\\frac{1}{14}$ |\n",
    "| `'I'` | $\\frac{1}{14}$ |\n",
    "| `'eat'` | $\\frac{1}{14}$ |\n",
    "| `'pizza'` | $\\frac{1}{14}$ |\n",
    "| `','` | $\\frac{1}{14}$ |\n",
    "| `'smile'` | $\\frac{1}{14}$ |\n",
    "| `'but'` | $\\frac{1}{14}$ |\n",
    "| `'drink'` | $\\frac{1}{14}$ |\n",
    "| `'Coke'` | $\\frac{1}{14}$ |\n",
    "| `'my'` | $\\frac{1}{14}$ |\n",
    "| `'stomach'` | $\\frac{1}{14}$ |\n",
    "| `'hurts'` | $\\frac{1}{14}$ |\n",
    "| `'\\x03'` | $\\frac{1}{14}$ |\n",
    "\n",
    "#### Unifrom Class:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, I pass in the \"training corpus\" on which my model will be trained. The `train` method uses that data to create a model which is saved in the `mdl` attribute. \n",
    "* The `train` method takes in a list of tokens and outputs a language model. **This language model is represented as a `Series`, whose index consists of tokens and whose values are the probabilities that the tokens occur.** \n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a positive integer `M` and generates a string made up of `M` tokens using the language model. **This method generates random sentences!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94df47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform Language Model\n",
    "class UniformLM(object):\n",
    "\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "\n",
    "        self.mdl = self.train(tokens)\n",
    "        \n",
    "    def train(self, tokens):\n",
    "        unique_tokens = pd.Series(tokens).unique()\n",
    "        return pd.Series(np.full(len(unique_tokens), 1/len(unique_tokens)), index=unique_tokens)\n",
    "    \n",
    "    def probability(self, words):\n",
    "        try:\n",
    "            words = list(words)\n",
    "            prob = np.prod(self.mdl.loc[words])\n",
    "        except KeyError:\n",
    "            prob = 0\n",
    "        return prob \n",
    "        \n",
    "    def sample(self, M):\n",
    "\n",
    "        return ' '.join(self.mdl.sample(M, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540af321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wealth rendered orchestras rest interminable Can property insisted cry boarder avoiding peculiarly Were Aren indian real Invilliers twinkle aunts opening sleeves Love settle breeze capes or divan closer Maintenon extraordinary smart sport drew sister measure movements beds hallway facet roughly introduce action in Read loneliness successful ring Wondering limit choking headed divine stored M spread thinks Nothing boisterously bastards squawk fragilely hue murmur carved reminded dirty blankly objects unusual gone special overhanging causing tattoo herding married sneakers thinning hesitated fingers Bird comprehended bosom Dewars wing Henry Ulysses Doubtless exciting effect undeserted waltz waiter bid which Buchanan general Daisy Meyer extreme'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Uniform Language Model on The Great Gatsby\n",
    "uniform = UniformLM(tokenized)\n",
    "uniform.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dccbbd",
   "metadata": {},
   "source": [
    "### Side Note 2: Uni-Gram Model\n",
    "A unigram language model is one in which the **probability assigned to a token is equal to the proportion of tokens in the corpus that are equal to said token**. That is, the probability distribution associated with a unigram language model is just the empirical distribution of tokens in the corpus. \n",
    "\n",
    "Let's understand how probabilities are assigned to tokens using our example corpus from before.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Here, there are 19 total tokens. 3 of them are equal to `'I'`, so $P(\\text{I}) = \\frac{3}{19}$. Here, the Series that `train` returns should contain the following values:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{19}$ |\n",
    "| `'when'` | $\\frac{2}{19}$ |\n",
    "| `'I'` | $\\frac{3}{19}$ |\n",
    "| `'eat'` | $\\frac{1}{19}$ |\n",
    "| `'pizza'` | $\\frac{1}{19}$ |\n",
    "| `','` | $\\frac{3}{19}$ |\n",
    "| `'smile'` | $\\frac{1}{19}$ |\n",
    "| `'but'` | $\\frac{1}{19}$ |\n",
    "| `'drink'` | $\\frac{1}{19}$ |\n",
    "| `'Coke'` | $\\frac{1}{19}$ |\n",
    "| `'my'` | $\\frac{1}{19}$ |\n",
    "| `'stomach'` | $\\frac{1}{19}$ |\n",
    "| `'hurts'` | $\\frac{1}{19}$ |\n",
    "| `'\\x03'` | $\\frac{1}{19}$ |\n",
    "\n",
    "As before, the `probability` method should take in a tuple and return its probability, using the probabilities stored in `mdl`. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')`. Then,\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\frac{2}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19} \\cdot \\frac{1}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edf177d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Uni-Gram Language Model\n",
    "class UnigramLM(object):\n",
    "    \n",
    "    def __init__(self, tokens):\n",
    "\n",
    "        self.mdl = self.train(tokens)\n",
    "    \n",
    "    def train(self, tokens):\n",
    "        tokens = pd.Series(tokens)\n",
    "        return tokens.value_counts().apply(lambda x: x/len(tokens))\n",
    "\n",
    "    def probability(self, words):\n",
    "        try:\n",
    "            words = list(words)\n",
    "            prob = np.prod(self.mdl.loc[words])\n",
    "        except KeyError:\n",
    "            prob = 0\n",
    "        return prob\n",
    "        \n",
    "    def sample(self, M):\n",
    "        return ' '.join(self.mdl.sample(M, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731f945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skins bounded now eyelashes straggled As temper walked Across before wisp polite keyed Gravely wanting understanding paper invent McKee cheekbone Montana remains amount Hasn heavens Whereupon grotesque page gardener spoke fianc some peered Very attired wasn cordials valued corridors Lewis desk unobtrusively gets spires burst adventurous is becomes feigned firm dream pungent unreal switch uncertainty delighted clever clog suffered enter kissed actually cries away cluster world brushed neared Jaqueline incomparable alive mysteries demoniac tells kind without funny bust several parents scales eastward twin crouching under pouring inquest Port trying cigarettes abandon guts lieutenant enjoined invariably clenched torpedoes fondled substitute formless'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Uni-Gram Language Model on The Great Gatsby\n",
    "\n",
    "unigram = UnigramLM(tokenized)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905af1a5",
   "metadata": {},
   "source": [
    "### Creating N-Gram Model\n",
    "\n",
    "The N-Gram language model relies on the assumption that only nearby tokens matter. Specifically, it assumes that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "In an N-Gram language model, there is a hyperparameter that we get to choose when creating the model, $N$. For any $N$, the resulting N-Gram model looks at the previous $N-1$ tokens when computing probabilities. (Note that the unigram model you built in Question 4 is really an N-Gram model with $N=1$, since it looked at 0 previous tokens when computing probabilities.)\n",
    "\n",
    "Both when working with a training corpus and when implementing the `probability` method to compute the probabilities of other sentences, I use  \"chunks\" of $N$ tokens at a time.\n",
    "\n",
    "**Definition:** The **N-Grams of a text** are a list of tuples containing sliding windows of length $N$.\n",
    "\n",
    "For instance, the trigrams in the sentence `'when I drink Coke I smile'` are:\n",
    "\n",
    "```py\n",
    "[('when', 'I', 'drink'), ('I', 'drink', 'Coke'), ('drink', 'Coke', 'I'), ('Coke', 'I', 'smile')]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Computing N-Gram Probabilities\n",
    "\n",
    "Notice in our trigram model above, I computed $P(\\text{when I drink Coke I smile})$ as being the product of several conditional probabilities. These conditional probabilities are the result of **training** our N-Gram model on a training corpus.\n",
    "\n",
    "To train an N-Gram model, I compute a conditional probability for every $N$-token sequence in the corpus. For instance, for every 3-token sequence $w_1, w_2, w_3$, I must compute $P(w_3 | w_1, w_2)$. To do so, I use:\n",
    "\n",
    "$$P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram sequence $w_1, w_2, w_3$ in the training corpus and $C(w_1, w_2)$ is the number of occurrences of the bigram sequence  $w_1, w_2$ in the training corpus. (Technical note: the probabilities that I compute using the ratios of counts are _estimates_ of the true conditional probabilities of N-Grams in the population of corpuses from which our corpus was drawn.)\n",
    "\n",
    "In general, for any $N$, conditional probabilities are computed by dividing the counts of N-Grams by the counts of the (N-1)-Grams they follow. \n",
    "\n",
    "<br>\n",
    "\n",
    "### The `NGramLM` Class\n",
    "\n",
    "The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and a positive integer `N`, specifying the N in N-grams. This parameter is stored in an attribute `N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-Grams (recall from above, an N-Gram is a **tuple** of length N). This list of N-Grams is then passed to the `train` method to train the N-Gram model.\n",
    "1. While the `train` method still creates a language model (in this case, an N-Gram model) and stores it in the `mdl` attribute, this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`, containing the N-Grams found in the text.\n",
    "    - `'n1gram'`, containing the (N-1)-Grams upon which the N-Grams in `ngram` are built.\n",
    "    - `'prob'`, containing the probabilities of each N-Gram in `ngram`.\n",
    "1. The `NGramLM` class has an attribute `prev_mdl` that stores an (N-1)-Gram language model over the same corpus (which in turn will store an (N-2)-Gram language model over the same corpus, and so on). This is necessary to compute the probability that a word occurs at the start of a text. \n",
    "\n",
    "N-Gram LM consists of probabilities of the form\n",
    "\n",
    "$$P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "Which can be estimated by:  \n",
    "\n",
    "$$\\frac{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1}, w_n)}{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1})}$$\n",
    "\n",
    "for every N-Gram that occurs in the corpus. To illustrate, consider again the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGrams(3, tokens)\n",
    "```\n",
    "\n",
    "Here, `pizza_model.train` must compute $P(\\text{I | \\x02 when})$, $P(\\text{eat | when I})$, $P(\\text{pizza | I eat})$, and so on, until $P(\\text{\\x03 | stomach hurts})$.\n",
    "\n",
    "To compute $P(\\text{eat | when I})$, I find the number of occurrences of `'when I eat'` in the training corpus, and divide it by the number of occurrences of `'when I'` in the training corpus. `'when I eat'` occurred exactly once in the training corpus, while `'when I'` occurred twice, so,\n",
    "\n",
    "$$P(\\text{eat | when I}) = \\frac{C(\\text{when I eat})}{C(\\text{when I})} = \\frac{1}{2}$$\n",
    "\n",
    "To store the conditional probabilities of all N-Grams, I use a DataFrame with three columns, like so:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ngram</th>\n",
    "      <th>n1gram</th>\n",
    "      <th>prob</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>(when, I, drink)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>(when, I, eat)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>(,, but, when)</td>\n",
    "      <td>(,, but)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>(,, I, smile)</td>\n",
    "      <td>(,, I)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>(I, smile, ,)</td>\n",
    "      <td>(I, smile)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>(,, my, stomach)</td>\n",
    "      <td>(,, my)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>(but, when, I)</td>\n",
    "      <td>(but, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>(\u0002, when, I)</td>\n",
    "      <td>(\u0002, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>(stomach, hurts, \u0003)</td>\n",
    "      <td>(stomach, hurts)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>(Coke, ,, my)</td>\n",
    "      <td>(Coke, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>(eat, pizza, ,)</td>\n",
    "      <td>(eat, pizza)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>(I, drink, Coke)</td>\n",
    "      <td>(I, drink)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>(my, stomach, hurts)</td>\n",
    "      <td>(my, stomach)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>(pizza, ,, I)</td>\n",
    "      <td>(pizza, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>(I, eat, pizza)</td>\n",
    "      <td>(I, eat)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>(drink, Coke, ,)</td>\n",
    "      <td>(drink, Coke)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>(smile, ,, but)</td>\n",
    "      <td>(smile, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "The row at position **1** in the above table shows that the probability of the trigram `('when', 'I', 'eat')` conditioned on the bigram `('when', 'I')` is 0.5, as we computed above. Note that many of the above conditional probabilities are equal to 1 because many trigrams and their corresponding bigrams each appeared only once, and $\\frac{1}{1} = 1$. Note that `'\\x02'` and `'\\x03'` appear as spaces above, such as in row **7**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "129c4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM(object):\n",
    "    \n",
    "    def __init__(self, N, tokens):\n",
    "        self.N = N\n",
    "        ngrams = self.create_ngrams(tokens)\n",
    "        self.tok = tokens\n",
    "        self.ngrams = ngrams\n",
    "        self.mdl = self.train(ngrams)\n",
    "        if N < 2:\n",
    "            raise Exception('N must be greater than 1')\n",
    "        elif N == 2:\n",
    "            self.prev_mdl = UnigramLM(tokens)\n",
    "        else:\n",
    "            self.prev_mdl = NGramLM(N-1, tokens)\n",
    "\n",
    "    def create_ngrams(self, tokens):\n",
    "        result = []\n",
    "        right_pointer = self.N\n",
    "        left_pointer = 0\n",
    "        \n",
    "        while right_pointer <= len(tokens):\n",
    "            result.append(tuple(tokens[left_pointer:right_pointer]))\n",
    "            left_pointer += 1\n",
    "            right_pointer += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_ngrams_N(self, n, tokens):\n",
    "        res = []\n",
    "        right = n\n",
    "        left = 0\n",
    "\n",
    "        while right <= len(tokens):\n",
    "            res.append(tuple(tokens[left:right]))\n",
    "            left += 1\n",
    "            right += 1\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def train(self, ngrams):\n",
    "        ser_ngram = pd.Series(ngrams, name=\"ngram\")\n",
    "        n1gram_create = self.create_ngrams_N(self.N-1, self.tok)\n",
    "        ser_n1gram = pd.Series(n1gram_create, name=\"n1gram\")\n",
    "        merge_ngram_n1gram = pd.merge(ser_ngram, ser_n1gram, left_index=True, right_index=True)\n",
    "        denom_col = merge_ngram_n1gram.groupby(\"n1gram\").transform('count')\n",
    "        numer_col = merge_ngram_n1gram.groupby(\"ngram\").transform('count')\n",
    "        merge_ngram_n1gram[\"numerator\"] = numer_col\n",
    "        merge_ngram_n1gram[\"denominator\"] = denom_col\n",
    "        merge_ngram_n1gram[\"prob\"] = merge_ngram_n1gram[\"numerator\"] / merge_ngram_n1gram[\"denominator\"]\n",
    "        \n",
    "        return merge_ngram_n1gram.drop(columns=[\"numerator\", \"denominator\"]).drop_duplicates(keep='first')\n",
    "    \n",
    "    def probability(self, words):\n",
    "        input_words = ' '.join(words)\n",
    "        token_words = ' '.join(self.tok)\n",
    "\n",
    "        if input_words not in token_words:\n",
    "            return 0\n",
    "        \n",
    "        final_prob = 1\n",
    "        \n",
    "        initial_prob = []\n",
    "        for i in range(self.N-1, 0, -1):\n",
    "            if i == 1:\n",
    "                initial_prob.append(words[0:i][0])\n",
    "            else:\n",
    "                initial_prob.append(words[0:i])\n",
    "        \n",
    "        list_grams = self.create_ngrams(words)\n",
    "        \n",
    "        for gram in list_grams: \n",
    "            if gram not in list(self.mdl['ngram'].values):\n",
    "                return 0\n",
    "            else:\n",
    "                final_prob *= self.mdl[self.mdl['ngram'] == gram]['prob'].iloc[0]\n",
    "                \n",
    "        current = self\n",
    "        \n",
    "        for gram in initial_prob:\n",
    "            current = current.prev_mdl\n",
    "            current_table = current.mdl\n",
    "            if isinstance(gram, str):\n",
    "                final_prob *= pd.DataFrame(current_table).loc[gram].iloc[0]\n",
    "            else:\n",
    "                final_prob *= current_table[current_table[\"ngram\"] == gram][\"prob\"].iloc[0]\n",
    "       \n",
    "        return final_prob\n",
    "        \n",
    "    def sample(self, M):\n",
    "        sample_words = ['\\x02']\n",
    "        \n",
    "        for i in range(1, self.N):\n",
    "            current = self.N - i\n",
    "            prev = self\n",
    "            \n",
    "            for i in range(current, 1, -1):\n",
    "                prev = prev.prev_mdl\n",
    "            \n",
    "            probability = prev.mdl[np.where(prev.mdl['n1gram'].apply(str) == str(tuple(sample_words)), True, False)].drop_duplicates(keep='first')\n",
    "            \n",
    "            if probability.shape[0] == 0 or probability.shape[1] == 0:\n",
    "                sample_words.append('\\x03')\n",
    "            else:\n",
    "                smalls = np.random.choice(probability['ngram'], p=probability['prob'])\n",
    "                sample_words.append(smalls[-1])\n",
    "               \n",
    "        for j in range(self.N, M+1):\n",
    "            probability = prev.mdl[np.where(prev.mdl['n1gram'].apply(str) == str(tuple(sample_words[-self.N+1:])), True, False)].drop_duplicates(keep='first')\n",
    "            \n",
    "            if probability.shape[0] == 0 or probability.shape[1] == 0:\n",
    "                sample_words.append('\\x03')\n",
    "            else:\n",
    "                words = np.random.choice(probability['ngram'], p=probability['prob'])\n",
    "                sample_words.append(np.random.choice(probability['ngram'], p=probability['prob'])[-1])\n",
    "               \n",
    "        if sample_words[-1] != '\\x03':\n",
    "            sample_words[-1] = '\\x03'\n",
    "            \n",
    "        return ' '.join(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de091f8d",
   "metadata": {},
   "source": [
    "## Part 4: Testing N-Gram Model\n",
    "<a name='part4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69dc9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize N-Gram Language Model on The Great Gatsby\n",
    "ngram = NGramLM(5, tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b783f629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x02', 'The', 'Great', 'Gatsby', 'by'),\n",
       " ('The', 'Great', 'Gatsby', 'by', 'F'),\n",
       " ('Great', 'Gatsby', 'by', 'F', '.'),\n",
       " ('Gatsby', 'by', 'F', '.', 'Scott'),\n",
       " ('by', 'F', '.', 'Scott', 'Fitzgerald'),\n",
       " ('F', '.', 'Scott', 'Fitzgerald', '\\x03'),\n",
       " ('.', 'Scott', 'Fitzgerald', '\\x03', '\\x02'),\n",
       " ('Scott', 'Fitzgerald', '\\x03', '\\x02', 'Table'),\n",
       " ('Fitzgerald', '\\x03', '\\x02', 'Table', 'of'),\n",
       " ('\\x03', '\\x02', 'Table', 'of', 'Contents')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In List Form\n",
    "ngram_list = ngram.create_ngrams(tokenized)\n",
    "ngram_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d17e8317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\u0002, The, Great, Gatsby, by)</td>\n",
       "      <td>(\u0002, The, Great, Gatsby)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(The, Great, Gatsby, by, F)</td>\n",
       "      <td>(The, Great, Gatsby, by)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Great, Gatsby, by, F, .)</td>\n",
       "      <td>(Great, Gatsby, by, F)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Gatsby, by, F, ., Scott)</td>\n",
       "      <td>(Gatsby, by, F, .)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(by, F, ., Scott, Fitzgerald)</td>\n",
       "      <td>(by, F, ., Scott)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ngram                    n1gram  prob\n",
       "0    (\u0002, The, Great, Gatsby, by)   (\u0002, The, Great, Gatsby)   1.0\n",
       "1    (The, Great, Gatsby, by, F)  (The, Great, Gatsby, by)   1.0\n",
       "2      (Great, Gatsby, by, F, .)    (Great, Gatsby, by, F)   1.0\n",
       "3      (Gatsby, by, F, ., Scott)        (Gatsby, by, F, .)   1.0\n",
       "4  (by, F, ., Scott, Fitzgerald)         (by, F, ., Scott)   1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Dataframe Form\n",
    "ngram_df = ngram.mdl\n",
    "ngram_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereate Sample Sentence using N-Gram Language Model on The Great Gatsby\n",
    "ngram_sample = ngram.sample(200) \n",
    "ngram_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05280236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117fcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdcb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2e162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
